{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten ways for text analysis\n",
    "1. Vectorizer\n",
    "2. TFIDF Vectorizer\n",
    "3. Sentiment Intensity Analyzer\n",
    "4. Data pre-processing: stopwords, word_tokenize\n",
    "5. PorterStemmer\n",
    "6. WordNetLemmatizer\n",
    "7. Word_tokenize\n",
    "8. FreqDist\n",
    "9. WordCloud\n",
    "10. Topic Modeling: LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vectorizer\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "1) tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "2) counting the occurrences of tokens in each document.\n",
    "3) normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "In this scheme, features and samples are defined as follows:\n",
    "1) each individual token occurrence frequency (normalized or not) is treated as a feature.\n",
    "2) the vector of all the token frequencies for a given document is considered a multivariate sample.\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Use bag of words for vectorization\n",
    "vectorizer = CountVectorizer()\n",
    "def vectorize_text(text):\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "[[1 1 2 1 1]]\n",
      "[[1 1 1 3]]\n",
      "[[1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one third third.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "for sen in corpus:\n",
    "    T = vectorize_text(sen)\n",
    "    print(T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default configuration tokenizes the string by extracting words of at least 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (\n",
    "    ['this', 'is', 'text', 'document', 'to', 'analyze'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Use TF-IDF for vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "def tfidf_vectorize_text(text):\n",
    "    X = tfidf_vectorizer.fit_transform([text])\n",
    "    return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4472136 0.4472136 0.4472136 0.4472136 0.4472136]]\n",
      "[[0.35355339 0.35355339 0.70710678 0.35355339 0.35355339]]\n",
      "[[0.28867513 0.28867513 0.28867513 0.8660254 ]]\n",
      "[[0.4472136 0.4472136 0.4472136 0.4472136 0.4472136]]\n",
      "[[0.9701425  0.24253563]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one third third.',\n",
    "    'Is this the first document?',\n",
    "    'Star Star Star Twinkle Star',\n",
    "]\n",
    "for sen in corpus:\n",
    "    T = tfidf_vectorize_text(sen)\n",
    "    print(T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sentiment Intensity Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) is a comprehensive Python library widely used for various Natural Language Processing (NLP) tasks. Here are some of the primary tasks and use cases for which NLTK is commonly applied:\n",
    "\n",
    "Tokenization:\n",
    "Breaking down text into words, sentences, or other meaningful units.\n",
    "Example: Splitting a paragraph into individual sentences or words.\n",
    "\n",
    "Text Normalization:\n",
    "Converting text to a standard format by processes like lowercasing, stemming, or lemmatization.\n",
    "Example: Reducing words to their base forms (e.g., \"running\" → \"run\").\n",
    "\n",
    "Stopwords Removal:\n",
    "Filtering out common words (such as \"and,\" \"the,\" \"is\") that may not add significant meaning to the analysis.\n",
    "\n",
    "Part-of-Speech (POS) Tagging:\n",
    "Labeling words with their corresponding parts of speech (nouns, verbs, adjectives, etc.), which is useful for syntactic and semantic analysis.\n",
    "\n",
    "Named Entity Recognition (NER):\n",
    "Identifying and classifying named entities (like persons, organizations, locations) within text.\n",
    "\n",
    "Parsing and Chunking:\n",
    "Analyzing the grammatical structure of sentences using techniques such as constituency parsing or dependency parsing.\n",
    "Example: Extracting noun or verb phrases from sentences.\n",
    "\n",
    "Corpus Management:\n",
    "Accessing and managing a variety of text corpora and lexical resources (e.g., the Brown Corpus, Gutenberg Corpus, or WordNet) for language research and model training.\n",
    "\n",
    "Text Classification:\n",
    "Building classifiers to categorize text into predefined classes, such as spam detection or sentiment analysis.\n",
    "\n",
    "Sentiment Analysis:\n",
    "Analyzing the sentiment or emotional tone of a piece of text. NLTK even includes tools like VADER for sentiment analysis in social media text.\n",
    "\n",
    "Language Modeling and N-grams:\n",
    "Creating statistical models based on n-grams (sequences of n words) to predict the next word in a sequence or analyze language patterns.\n",
    "\n",
    "Educational Purposes:\n",
    "NLTK is also heavily used in academia for teaching and research in computational linguistics and NLP because of its ease of use and comprehensive documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Tokenization and POS Tagging\n",
    "\n",
    "Common POS Tags and Their Meanings\n",
    "NLTK uses the Penn Treebank tag set by default. Some of the most common tags include:\n",
    "\n",
    "CC: Coordinating conjunction (e.g., and, but, or)\n",
    "CD: Cardinal number (e.g., one, two, 3, 100)\n",
    "DT: Determiner (e.g., the, a, an)\n",
    "EX: Existential there (e.g., there is, there are)\n",
    "FW: Foreign word (e.g., non-English words in an English text)\n",
    "IN: Preposition or subordinating conjunction (e.g., in, on, at, because)\n",
    "JJ: Adjective (e.g., blue, quick, large)\n",
    "JJR: Adjective, comparative (e.g., better, larger)\n",
    "JJS: Adjective, superlative (e.g., best, largest)\n",
    "LS: List item marker (e.g., numbered items or bullet points)\n",
    "MD: Modal (e.g., can, might, should)\n",
    "NN: Noun, singular or mass (e.g., dog, car, music)\n",
    "NNS: Noun, plural (e.g., dogs, cars)\n",
    "NNP: Proper noun, singular (e.g., London, Alice)\n",
    "NNPS: Proper noun, plural (e.g., Americans, Beatles)\n",
    "PDT: Predeterminer (e.g., all, both, half)\n",
    "POS: Possessive ending (e.g., ’s in “John’s”)\n",
    "PRP: Personal pronoun (e.g., I, you, he, she)\n",
    "PRP$: Possessive pronoun (e.g., my, your, his, her)\n",
    "RB: Adverb (e.g., quickly, silently)\n",
    "RBR: Adverb, comparative (e.g., faster)\n",
    "RBS: Adverb, superlative (e.g., fastest)\n",
    "RP: Particle (e.g., up, off, out when used with verbs such as “pick up”)\n",
    "TO: The word “to” (e.g., in the infinitive “to go”)\n",
    "UH: Interjection (e.g., oh, uh, wow)\n",
    "VB: Verb, base form (e.g., run, eat)\n",
    "VBD: Verb, past tense (e.g., ran, ate)\n",
    "VBG: Verb, gerund or present participle (e.g., running, eating)\n",
    "VBN: Verb, past participle (e.g., run, eaten)\n",
    "VBP: Verb, non-3rd person singular present (e.g., run, eat)\n",
    "VBZ: Verb, 3rd person singular present (e.g., runs, eats)\n",
    "WDT: Wh-determiner (e.g., which, that)\n",
    "WP: Wh-pronoun (e.g., who, what)\n",
    "WP$: Possessive wh-pronoun (e.g., whose)\n",
    "WRB: Wh-adverb (e.g., how, where, when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # For tokenizers\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample text\n",
    "text = \"NLTK is a powerful library for natural language processing.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Perform POS tagging on the tokens\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    return sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.421, 'pos': 0.579, 'compound': 0.6697}\n",
      "{'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.4404}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one third third.',\n",
    "    'Is this the first document?',\n",
    "    'I am very excited about this opportunity',\n",
    "    'This could be better if the timing was right',\n",
    "]\n",
    "for sen in corpus:\n",
    "    T = sentiment_analysis(sen)\n",
    "    print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data pre-processing: stopwords, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Data preprocessing to remove puncturation and stopwords\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_word = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first document\n",
      "second second document\n",
      "third one third third\n",
      "first document\n",
      "excited opportunity\n",
      "could better timing right\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one third third.',\n",
    "    'Is this the first document?',\n",
    "    'I am very excited about this opportunity',\n",
    "    'This could be better if the timing was right',\n",
    "]\n",
    "for sen in corpus:\n",
    "    T = preprocess_text(sen)\n",
    "    print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'the', 'first', 'document.']\n",
      "['thi', 'is', 'the', 'second', 'second', 'document.']\n",
      "['and', 'the', 'third', 'one', 'third', 'third.']\n",
      "['is', 'thi', 'the', 'first', 'document?']\n",
      "['i', 'am', 'veri', 'excit', 'about', 'thi', 'opportun']\n",
      "['thi', 'could', 'be', 'better', 'if', 'the', 'time', 'wa', 'right']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one third third.',\n",
    "    'Is this the first document?',\n",
    "    'I am very excited about this opportunity',\n",
    "    'This could be better if the timing was right',\n",
    "]\n",
    "\n",
    "#text = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
    "stem_words(text)\n",
    "\n",
    "for sen in corpus:\n",
    "    T = stem_words(sen.split())\n",
    "    print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'wa', 'the', 'first', 'document.']\n",
      "['This', 'is', 'the', 'second', 'second', 'document.']\n",
      "['And', 'the', 'third', 'one', 'third', 'third.']\n",
      "['Is', 'this', 'the', 'first', 'document?']\n",
      "['I', 'am', 'very', 'excited', 'about', 'this', 'opportunity']\n",
      "['This', 'could', 'be', 'better', 'if', 'the', 'timing', 'wa', 'right']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This was the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one third third.',\n",
    "    'Is this the first document?',\n",
    "    'I am very excited about this opportunity',\n",
    "    'This could be better if the timing was right',\n",
    "]\n",
    "for sen in corpus:\n",
    "    T = lemmatize_words(sen.split())\n",
    "    print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: running -> Lemma: run\n",
      "Original: runs -> Lemma: run\n",
      "Original: ran -> Lemma: easily\n",
      "Original: easily -> Lemma: fairly\n",
      "Original: fairly -> Lemma: good\n"
     ]
    }
   ],
   "source": [
    "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
    "\n",
    "lemmatized_words = [\n",
    "    lemmatizer.lemmatize(\"running\", pos=wordnet.VERB),\n",
    "    lemmatizer.lemmatize(\"ran\", pos=wordnet.VERB),\n",
    "    lemmatizer.lemmatize(\"easily\", pos=wordnet.ADV),\n",
    "    lemmatizer.lemmatize(\"fairly\", pos=wordnet.ADV),\n",
    "    lemmatizer.lemmatize(\"better\", pos=wordnet.ADJ)\n",
    "]\n",
    "\n",
    "# Print the original words and their lemmatized forms\n",
    "for word, lemma in zip(words, lemmatized_words):\n",
    "    print(f\"Original: {word} -> Lemma: {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'the', 'first', 'document', '.']\n",
      "['This', 'is', 'the', 'second', 'second', 'document', '.']\n",
      "['And', 'the', 'third', 'one', 'third', 'third', '.']\n",
      "['Is', 'this', 'the', 'first', 'document', '?']\n",
      "['I', 'am', 'very', 'excited', 'about', 'this', 'opportunity']\n",
      "['This', 'could', 'be', 'better', 'if', 'the', 'timing', 'was', 'right']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This was the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one third third.',\n",
    "    'Is this the first document?',\n",
    "    'I am very excited about this opportunity',\n",
    "    'This could be better if the timing was right',\n",
    "]\n",
    "for sen in corpus:\n",
    "    T = tokenize(sen)\n",
    "    print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def calculate_frequency_distribution(words):\n",
    "    freq_dist = FreqDist(words)\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 5 samples and 5 outcomes>\n",
      "<FreqDist with 5 samples and 6 outcomes>\n",
      "<FreqDist with 5 samples and 6 outcomes>\n",
      "<FreqDist with 5 samples and 5 outcomes>\n",
      "<FreqDist with 7 samples and 7 outcomes>\n",
      "<FreqDist with 9 samples and 9 outcomes>\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This was the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one third third.',\n",
    "    'Is this the first document?',\n",
    "    'I am very excited about this opportunity',\n",
    "    'This could be better if the timing was right',\n",
    "]\n",
    "for sen in corpus:\n",
    "    T = calculate_frequency_distribution(sen.split())\n",
    "    print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_wordcloud(frequencies):\n",
    "    wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(frequencies)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Topic Modeling: LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "\n",
    "def topic_modeling(documents):\n",
    "    return lda.fit_transform(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
